{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../py/')\n",
    "\n",
    "from simplify import Nouns, PostfixNouns, process, MergeEomiJosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('내', 'MM'), ('초코우유', 'Noun'), ('는', 'J')]\n",
      "[('내초코우유', 'Noun'), ('에', 'J'), ('는', 'J')]\n",
      "[('내초코우유', 'Noun'), ('에는', 'Josa')]\n",
      "[('내초코우유', 'Noun'), ('는', 'Josa')]\n",
      "[('내제3초코우유', 'Noun'), ('는', 'Josa')]\n",
      "[('내초코', 'Noun'), ('같', 'Adjective'), ('은', 'Eomi')]\n",
      "[('내초코', 'Noun'), ('먹', 'Verb'), ('은', 'Eomi')]\n",
      "[('내초코', 'Noun'), ('아니', 'Verb'), ('다', 'Eomi')]\n",
      "[('안먹', 'Verb'), ('는다', 'Eomi')]\n",
      "[('사실', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')]\n",
      "[('개초초대박', 'Noun'), ('이', 'Adjective'), ('ㄴ데', 'Eomi')]\n",
      "[('동원', 'Noun'), ('해서', 'Adjective'), ('라도', 'Eomi')]\n",
      "[('내제3초코우유', 'Noun'), ('는', 'Josa')]\n",
      "[['있', 'Verb'], ('다고', 'Eomi')]\n"
     ]
    }
   ],
   "source": [
    "print(Nouns([('내', 'MM'), ('초코','NN'), ('우유', 'NN'), ('는', 'J')]))\n",
    "print(PostfixNouns([('내', 'MM'), ('초코','NN'), ('우유', 'NN'), ('에', 'J'), ('는', 'J')]))\n",
    "print(process([('내', 'MM'), ('초코','NN'), ('우유', 'NN'), ('에', 'J'), ('는', 'J')]))\n",
    "print(process([('내', 'MM'), ('초코','NN'), ('우유', 'NN'), ('는', 'J')]))\n",
    "print(process([('내', 'MM'), ('제', 'XPN'), ('3', 'SN'), ('초코','NN'), ('우유', 'NN'), ('는', 'J')]))\n",
    "print(process([('내', 'MM'), ('초코','NN'), ('같', 'XSA'), ('은', 'E')]))\n",
    "print(process([('내', 'MM'), ('초코','NN'), ('먹', 'XSV'), ('은', 'E')]))\n",
    "print(process([('내', 'MM'), ('초코','NN'), ('아니', 'VCN'), ('다', 'E')]))\n",
    "print(process([('안', 'MAG'), ('먹', 'VV'), ('는', 'E'), ('다', 'E')]))\n",
    "print(process([('사실', 'NNG'), ('이', 'VCP'), ('다', 'E')]))\n",
    "print(process([('개', 'MM'), ('초', 'MM'), ('초', 'MM'), ('대박', 'NNG'), ('이', 'VCP'), ('ㄴ데', 'EP')]))\n",
    "print(process([['동원', 'NNG'], ['하', 'XSV'], ['아서', 'EC'], ['이', 'VCP'], ['라도', 'EC']]))\n",
    "print(process([('\"', 'SP'), ('내', 'MM'), ('제', 'XPN'), ('3', 'SN'), ('초코','NN'), ('우유', 'NN'), ('\"', 'SP'), ('는', 'J'), (',', 'SP')]))\n",
    "print(MergeEomiJosa([['있', 'Verb'], ['다', 'Eomi'], ['고', 'Josa']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['것', 'NNB'], ['이', 'VCP'], ['다', 'EF'], ['.', 'SF']]\n",
      "[('것', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')] \n",
      "\n",
      "[['있', 'VX'], ['다', 'EF'], ['.', 'SF']]\n",
      "[('있', 'Verb'), ('다', 'Eomi')] \n",
      "\n",
      "[['있', 'VX'], ['는', 'ETM']]\n",
      "[('있', 'Verb'), ('는', 'Eomi')] \n",
      "\n",
      "[['있', 'VV'], ['다', 'EF'], ['.', 'SF']]\n",
      "[('있', 'Verb'), ('다', 'Eomi')] \n",
      "\n",
      "[['있', 'VV'], ['는', 'ETM']]\n",
      "[('있', 'Verb'), ('는', 'Eomi')] \n",
      "\n",
      "[['것', 'NNB'], ['은', 'JX']]\n",
      "[('것', 'Noun'), ('은', 'Josa')] \n",
      "\n",
      "[['같', 'VA'], ['은', 'ETM']]\n",
      "[('같', 'Adjective'), ('은', 'Eomi')] \n",
      "\n",
      "[['나', 'NP'], ['는', 'JX']]\n",
      "[('나', 'Noun'), ('는', 'Josa')] \n",
      "\n",
      "[['대하', 'VV'], ['ᆫ', 'ETM']]\n",
      "[('대하', 'Verb'), ('ᆫ', 'Eomi')] \n",
      "\n",
      "[['것', 'NNB'], ['이', 'JKS']]\n",
      "[('것', 'Noun'), ('이', 'Josa')] \n",
      "\n",
      "[['하', 'VV'], ['는', 'ETM']]\n",
      "[('하', 'Verb'), ('는', 'Eomi')] \n",
      "\n",
      "[['것', 'NNB'], ['을', 'JKO']]\n",
      "[('것', 'Noun'), ('을', 'Josa')] \n",
      "\n",
      "[['그', 'NP'], ['는', 'JX']]\n",
      "[('그', 'Noun'), ('는', 'Josa')] \n",
      "\n",
      "[['없', 'VA'], ['는', 'ETM']]\n",
      "[('없', 'Adjective'), ('는', 'Eomi')] \n",
      "\n",
      "[['있', 'VX'], ['었', 'EP'], ['다', 'EF'], ['.', 'SF']]\n",
      "[('있', 'Verb'), ('었다', 'Eomi')] \n",
      "\n",
      "[['하', 'VX'], ['ᆫ다', 'EF'], ['.', 'SF']]\n",
      "[('하', 'Verb'), ('ᆫ다', 'Eomi')] \n",
      "\n",
      "[['내', 'NP'], ['가', 'JKS']]\n",
      "[('내', 'Noun'), ('가', 'Josa')] \n",
      "\n",
      "[['하', 'VV'], ['ᆯ', 'ETM']]\n",
      "[('하', 'Verb'), ('ᆯ', 'Eomi')] \n",
      "\n",
      "[['하', 'VV'], ['고', 'EC']]\n",
      "[('하', 'Verb'), ('고', 'Eomi')] \n",
      "\n",
      "[['때문', 'NNB'], ['에', 'JKB']]\n",
      "[('때문', 'Noun'), ('에', 'Josa')] \n",
      "\n",
      "[['그', 'NP'], ['의', 'JKG']]\n",
      "[('그', 'Noun'), ('의', 'Josa')] \n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "n_error = 0\n",
    "n_print = 0\n",
    "with open('../../sejong/data/processed/lr/tokentable.txt', encoding='utf-8') as fi:\n",
    "    with open('../../sejong/data/processed/lr/tokentable_simple.txt', 'w', encoding='utf-8') as fo:\n",
    "        for i, line in enumerate(fi):\n",
    "            col = line.split('\\t')\n",
    "            if not (len(col) == 3):\n",
    "                continue\n",
    "            pos = col[1].split('+')\n",
    "            pos = [p for p in pos if p and not (p[0] == '/')]\n",
    "            pos = [p.split('/') for p in pos]\n",
    "            pos = [p[:2] for p in pos]\n",
    "            pos = [p for p in pos if p and len(p) == 2]            \n",
    "            pos = [p for p in pos if not ((p[1][0] == 'S' or p[1][0] == 'U') and p[1] != 'SN')]\n",
    "            \n",
    "            if not pos:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                pos = process(pos)\n",
    "                \n",
    "                if 2 <= len(pos) <= 4 and n_print <= 20:\n",
    "                    n_print += 1\n",
    "                    print([p.split('/') for p in col[1].split('+')])\n",
    "                    print(pos, '\\n')\n",
    "                for word,tag in pos:\n",
    "                    counter[tag][word] += 1\n",
    "                    \n",
    "                pos = '+'.join(['{}/{}'.format(w,t) for w,t in pos])\n",
    "                fo.write('{}\\t{}\\t{}\\t{}\\n'.format(col[0], col[1], pos, col[2].strip()))\n",
    "                \n",
    "            except Exception as e: \n",
    "                n_error += 1\n",
    "            \n",
    "print(n_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun has 378946 words\n",
      "Eomi has 24752 words\n",
      "Verb has 7476 words\n",
      "Adverb has 6049 words\n",
      "Adjective has 4968 words\n",
      "Josa has 3317 words\n",
      "Exclamation has 1198 words\n",
      "Determiner has 206 words\n",
      "SN has 4 words\n"
     ]
    }
   ],
   "source": [
    "for tag, unique_words in sorted(counter.items(), key=lambda x:-len(x[1])):\n",
    "    print('{} has {} words'.format(tag, len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
